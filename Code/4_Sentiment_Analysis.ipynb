{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import expanded reg sentences\n",
    "df_regSentsExpand=pd.read_pickle('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Dec2021/RegSentsExpand_NounChunks.pkl')\n",
    "print(df_regSentsExpand.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Refine to reg relevant articles\n",
    "# df=df_regSentsExpand[df_regSentsExpand['NounChunkMatchFiltered']>0].reset_index(drop=True)\n",
    "df=df_regSentsExpand.reset_index(drop=True)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negation words\n",
    "negate = [\"aint\", \"arent\", \"cannot\", \"cant\", \"couldnt\", \"darent\", \"didnt\", \"doesnt\", \"ain't\", \"aren't\", \"can't\",\n",
    "          \"couldn't\", \"daren't\", \"didn't\", \"doesn't\", \"dont\", \"hadnt\", \"hasnt\", \"havent\", \"isnt\", \"mightnt\", \"mustnt\",\n",
    "          \"neither\", \"don't\", \"hadn't\", \"hasn't\", \"haven't\", \"isn't\", \"mightn't\", \"mustn't\", \"neednt\", \"needn't\",\n",
    "          \"never\", \"none\", \"nope\", \"nor\", \"not\", \"nothing\", \"nowhere\", \"oughtnt\", \"shant\", \"shouldnt\", \"wasnt\",\n",
    "          \"werent\", \"oughtn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"without\", \"wont\", \"wouldnt\", \"won't\",\n",
    "          \"wouldn't\", \"rarely\", \"seldom\", \"despite\", \"no\", \"nobody\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to negate\n",
    "def negated(word):\n",
    "    # Determine if preceding word is a negation word\n",
    "    if word.lower() in negate:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to tokenize\n",
    "def tokenizer(text):\n",
    "    doc=nlp(text)\n",
    "    tokens=[token.text.lower() for token in doc if not token.is_punct | token.is_space]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to lemmatize\n",
    "def lemmatizer(text):\n",
    "    doc=nlp(text)\n",
    "    lemmas=[token.lemma_ for token in doc if not token.is_punct | token.is_space]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. LM uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LM dictionary\n",
    "LMlist=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Aug2020/Sentiment Analysis/LoughranMcDonald_SentimentList.csv')\n",
    "print(LMlist.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LM uncertainty dictionary\n",
    "LMuncertain=LMlist[LMlist['Uncertainty'].notnull()]['Uncertainty'].tolist()\n",
    "uncertaindict={'Uncertainty': [w.lower() for w in LMuncertain]}\n",
    "#print(uncertaindict, len(LMuncertain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Lemmatize LM uncertainty dictionary\n",
    "uncertainset=set()\n",
    "for w in uncertaindict['Uncertainty']:\n",
    "    v=''.join(lemmatizer(w))\n",
    "    uncertainset.add(v)\n",
    "uncertainlist_lemmatized=list(uncertainset)\n",
    "print(len(uncertainlist_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncertainlist_lemmatized[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count uncertainty terms\n",
    "def uncertainty_count(keywords_list, article):\n",
    "\n",
    "    uncertain_count = 0\n",
    "    uncertain_words = []\n",
    " \n",
    "    input_words=lemmatizer(article)\n",
    "    word_count = len(input_words)\n",
    "    \n",
    "    for i in range(0, word_count):\n",
    "        if input_words[i] in keywords_list:\n",
    "            uncertain_count += 1\n",
    "            uncertain_words.append(input_words[i])\n",
    "    \n",
    "    results = [uncertain_count, uncertain_words]\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LM uncertainty through all expanded reg sentences\n",
    "UncertaintyCount=[]\n",
    "UncertaintyWords=[]\n",
    "for text in df['RegSentsExpand']:\n",
    "    results=uncertainty_count(uncertainlist_lemmatized, text)\n",
    "    UncertaintyCount.append(results[0])\n",
    "    UncertaintyWords.append(results[1])\n",
    "print(len(UncertaintyCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['UncertaintyCount']=UncertaintyCount\n",
    "df['UncertaintyWords']=UncertaintyWords\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['UncertaintyCount']!=0]['ID'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['ID','UncertaintyCount','UncertaintyWords']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','UncertaintyCount','UncertaintyWords']].to_csv('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Dec2021/LMuncertainty.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. LM sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count sentiment terms\n",
    "def sentiment_count(dict, article):\n",
    "    \"\"\"\n",
    "    Count positive and negative words with negation check. Account for simple negation only for positive words.\n",
    "    Simple negation is taken to be observations of one of negate words occurring within three words\n",
    "    preceding a positive words.\n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    " \n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    " \n",
    "    input_words=lemmatizer(article)\n",
    " \n",
    "    word_count = len(input_words)\n",
    " \n",
    "    for i in range(0, word_count):\n",
    "        if input_words[i] in dict['Negative']:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                neg_count += 1\n",
    "                neg_words.append(input_words[i])\n",
    "            \n",
    "        if input_words[i] in dict['Positive']:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                pos_count += 1\n",
    "                pos_words.append(input_words[i])\n",
    "    '''\n",
    "    print('The results with negation check:', end='\\n\\n')\n",
    "    print('The # of positive words:', pos_count)\n",
    "    print('The # of negative words:', neg_count)\n",
    "    print('The list of found positive words:', pos_words)\n",
    "    print('The list of found negative words:', neg_words)\n",
    "    print('\\n', end='')\n",
    "    '''\n",
    "    \n",
    "    results = [word_count, pos_count, neg_count, pos_words, neg_words]\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LM sentiment dictionary\n",
    "LMposWords=LMlist[LMlist['Positive'].notnull()]['Positive'].tolist()\n",
    "LMnegWords=LMlist[LMlist['Negative'].notnull()]['Negative'].tolist()\n",
    "print(len(LMnegWords),len(LMposWords))\n",
    "print(LMnegWords[0:20],LMposWords[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize LM sentiment dictionary\n",
    "LMnegset=set()\n",
    "for w in LMnegWords:\n",
    "    v=''.join(lemmatizer(w.lower()))\n",
    "    LMnegset.add(v)\n",
    "print(len(LMnegset))\n",
    "\n",
    "LMposset=set()\n",
    "for w in LMposWords:\n",
    "    v=''.join(lemmatizer(w.lower()))\n",
    "    LMposset.add(v)\n",
    "print(len(LMposset))\n",
    "\n",
    "LMdict={'Negative': list(LMnegset), 'Positive': list(LMposset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LMdict['Positive'][0:20])\n",
    "print(LMdict['Negative'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LM sentiment through all expanded reg sentences\n",
    "LMpositiveCount=[]\n",
    "LMnegativeCount=[]\n",
    "LMpositiveWords=[]\n",
    "LMnegativeWords=[]\n",
    "for text in df['RegSentsExpand']:\n",
    "    results=sentiment_count(LMdict, text)\n",
    "    LMpositiveCount.append(results[1])\n",
    "    LMnegativeCount.append(results[2])\n",
    "    LMpositiveWords.append(results[3])\n",
    "    LMnegativeWords.append(results[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LMposCount']=LMpositiveCount\n",
    "df['LMnegCount']=LMnegativeCount\n",
    "df['LMposWords']=LMpositiveWords\n",
    "df['LMnegWords']=LMnegativeWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','LMposCount','LMnegCount','LMposWords','LMnegWords']].to_csv('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Dec2021/LMsentiments.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. GI sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harvard GI sentiment dictionary\n",
    "with open(\"/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Aug2020/Sentiment Analysis/GIposWords.txt\", \"rb\") as fp:   # Unpickling\n",
    "    GIposWords = pickle.load(fp)\n",
    "with open(\"/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Aug2020/Sentiment Analysis/GInegWords.txt\", \"rb\") as fp:   # Unpickling\n",
    "    GInegWords = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(GIposWords),GIposWords[0:20])\n",
    "print(len(GInegWords),GInegWords[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-lemmetized version of GI dictionary\n",
    "GIdict2={'Negative': [w.lower() for w in GInegWords], 'Positive': [w.lower() for w in GIposWords]}\n",
    "print('Positive:',GIdict2['Positive'][0:20])\n",
    "print('Negative:',GIdict2['Negative'][0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run GI sentiment through all expanded reg sentences using non-lemmatized GI dictionary (performs better than lemmatized GI)\n",
    "totalWordCount=[]\n",
    "GIpositiveCount=[]\n",
    "GInegativeCount=[]\n",
    "GIpositiveWords=[]\n",
    "GInegativeWords=[]\n",
    "for text in df['RegSentsExpand']:\n",
    "    results=sentiment_count(GIdict2, text)\n",
    "    totalWordCount.append(results[0])\n",
    "    GIpositiveCount.append(results[1])\n",
    "    GInegativeCount.append(results[2])\n",
    "    GIpositiveWords.append(results[3])\n",
    "    GInegativeWords.append(results[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalWordCount']=totalWordCount\n",
    "df['GIposCount']=GIpositiveCount\n",
    "df['GInegCount']=GInegativeCount\n",
    "df['GIposWords']=GIpositiveWords\n",
    "df['GInegWords']=GInegativeWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','TotalWordCount','GIposCount','GInegCount','GIposWords','GInegWords']].to_csv('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Dec2021/GIsentiments.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. LSD sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicoder Sentiment Dictionary (LSD)\n",
    "LSDlist=pd.read_csv('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Aug2020/Sentiment Analysis/LSDsentimentWords_wStar.csv')\n",
    "print(LSDlist.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSDneg=LSDlist[LSDlist['LSDnegative'].notnull()]['LSDnegative'].tolist()\n",
    "LSDpos=LSDlist[LSDlist['LSDpositive'].notnull()]['LSDpositive'].tolist()\n",
    "LSDdict={'Negative': [w.lower() for w in LSDneg], 'Positive': [w.lower() for w in LSDpos]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate terms with & without stars in LSD dictionary\n",
    "pos_star=[]\n",
    "pos_nostar=[]\n",
    "for m in LSDdict['Positive']:\n",
    "    if \"*\" in m:\n",
    "        m=m.replace('*','')\n",
    "        pos_star.append(m)\n",
    "    else:\n",
    "        pos_nostar.append(m)\n",
    "print(len(pos_star), len(pos_nostar))\n",
    "\n",
    "neg_star=[]\n",
    "neg_nostar=[]\n",
    "for m in LSDdict['Negative']:\n",
    "    if \"*\" in m:\n",
    "        m=m.replace('*','')\n",
    "        neg_star.append(m)\n",
    "    else:\n",
    "        neg_nostar.append(m)\n",
    "print(len(neg_star), len(neg_nostar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile re patterns for terms with & without stars\n",
    "pattern_pos_nostar=re.compile(r'\\b(?:%s)\\b' % '|'.join(pos_nostar))\n",
    "pattern_pos_star=re.compile(r'\\b(?:%s)[a-zA-Z]*\\b' % '|'.join(pos_star))\n",
    "pattern_neg_nostar=re.compile(r'\\b(?:%s)\\b' % '|'.join(neg_nostar))\n",
    "pattern_neg_star=re.compile(r'\\b(?:%s)[a-zA-Z]*\\b' % '|'.join(neg_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count LSD sentiment terms\n",
    "def LSDsentiment_count(dict, article):\n",
    "    \"\"\"\n",
    "    Count positive and negative words with negation check. Account for simple negation only for positive words.\n",
    "    Simple negation is taken to be observations of one of negate words occurring within three words\n",
    "    preceding a positive words.\n",
    "    \"\"\"\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    " \n",
    "    pos_words = []\n",
    "    neg_words = []\n",
    " \n",
    "    input_words=tokenizer(article)    # No lemmatizing since LSD dictionary includes variations\n",
    " \n",
    "    word_count = len(input_words)\n",
    " \n",
    "    for i in range(0, word_count):\n",
    "        find_neg=pattern_neg_nostar.findall(input_words[i])+pattern_neg_star.findall(input_words[i])\n",
    "        if len(find_neg)>0:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                neg_count += 1\n",
    "                neg_words.append(input_words[i])\n",
    "        \n",
    "        find_pos=pattern_pos_nostar.findall(input_words[i])+pattern_pos_star.findall(input_words[i])\n",
    "        if len(find_pos)>0:\n",
    "            if i >= 3:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]) or negated(input_words[i - 3]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 2:\n",
    "                if negated(input_words[i - 1]) or negated(input_words[i - 2]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 1:\n",
    "                if negated(input_words[i - 1]):\n",
    "                    neg_count += 1\n",
    "                    neg_words.append(input_words[i] + ' (with negation)')\n",
    "                else:\n",
    "                    pos_count += 1\n",
    "                    pos_words.append(input_words[i])\n",
    "            elif i == 0:\n",
    "                pos_count += 1\n",
    "                pos_words.append(input_words[i])\n",
    "    '''\n",
    "    print('The results with negation check:', end='\\n\\n')\n",
    "    print('The # of positive words:', pos_count)\n",
    "    print('The # of negative words:', neg_count)\n",
    "    print('The list of found positive words:', pos_words)\n",
    "    print('The list of found negative words:', neg_words)\n",
    "    print('\\n', end='')\n",
    "    '''\n",
    "    \n",
    "    results = [word_count, pos_count, neg_count, pos_words, neg_words]\n",
    " \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LSD sentiment through all expanded reg sentences\n",
    "start_time = time.time()\n",
    "\n",
    "LSDpositiveCount=[]\n",
    "LSDnegativeCount=[]\n",
    "LSDpositiveWords=[]\n",
    "LSDnegativeWords=[]\n",
    "failed=[]\n",
    "for i in range(0, len(df['RegSentsExpand'])):\n",
    "    try:\n",
    "        results=LSDsentiment_count(LSDdict, df['RegSentsExpand'][i])\n",
    "    except:\n",
    "        results=[None, None, None, None, None]\n",
    "        failed.append(i)        \n",
    "        \n",
    "    LSDpositiveCount.append(results[1])\n",
    "    LSDnegativeCount.append(results[2])\n",
    "    LSDpositiveWords.append(results[3])\n",
    "    LSDnegativeWords.append(results[4])\n",
    "print(len(failed))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(failed))\n",
    "print(len(LSDpositiveWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LSDposCount']=LSDpositiveCount\n",
    "df['LSDnegCount']=LSDnegativeCount\n",
    "df['LSDposWords']=LSDpositiveWords\n",
    "df['LSDnegWords']=LSDnegativeWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    print(df['RegSentsExpand'][i],df['LSDposWords'][i],df['LSDnegWords'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ID','LSDposCount','LSDnegCount','LSDposWords','LSDnegWords']].to_csv('/home/ec2-user/SageMaker/New Uncertainty/Jan1985-Dec2021/LSDsentiments.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py",
   "language": "python",
   "name": "my_py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
